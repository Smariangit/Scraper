{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0fba52-b784-4fa8-b35f-4632b9e38162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "import openpyxl as op\n",
    "import pandas as pd\n",
    "\n",
    "import pyphen\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5849c9b5-1809-4100-bc0b-24a97e352180",
   "metadata": {},
   "source": [
    "## The function **text_extractor** scrapes a webpage and outputs text data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0b315f-186e-4816-ae2b-25fddb7dad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(url, title):\n",
    "    res=requests.get(url)\n",
    "    h=res.content\n",
    "    soup=BeautifulSoup(h, 'html.parser')\n",
    "    \n",
    "    sx=''\n",
    "    t=''\n",
    "    slt=soup.find_all('p')\n",
    "    \n",
    "    for a in slt:\n",
    "        if a.has_attr('title') or a.has_attr('id') or a.has_attr('class') or a.has_attr('href') or a.has_attr('strong'):\n",
    "            continue\n",
    "        else:\n",
    "            sx+=(str(a))+'\\t'\n",
    "\n",
    "    t=BeautifulSoup(sx, features='html.parser').text\n",
    "\n",
    "    g=open(title, 'w',encoding='utf-8')\n",
    "    g.write(t)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec9851-d7f1-41d4-918b-2247fc2fd3b2",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27bf73-4c03-45ce-b5c7-9de12df6ef24",
   "metadata": {},
   "source": [
    "## reading positive and negative dictionary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29de1774-a821-4630-9f64-efce6d7226e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Docs/positive-words.txt','r') as file:\n",
    "    pos=file.read().split()\n",
    "\n",
    "with open('Docs/negative-words.txt','r') as file:\n",
    "    neg=file.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732939c2-680f-43b7-826d-5ee059d342f5",
   "metadata": {},
   "source": [
    "### **remove_common_words** gives the following analysed values after removing the common words-\n",
    "- personal pronoun count\n",
    "- word count\n",
    "- positive words\n",
    "- negative words\n",
    "- polarity score\n",
    "- subjectivity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc46302-e32d-4afd-834e-aede7d214024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listify(filename):\n",
    "    try:\n",
    "        \n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            words = file.read().split()\n",
    "        if len(words)==0:\n",
    "            with open(filename, 'r') as file:\n",
    "                words = file.read().split()\n",
    "        return set(words)\n",
    "        \n",
    "    except UnicodeEncodeError:\n",
    "        print('FacedError')\n",
    "        with open(filename, 'r') as file:\n",
    "            words = file.read().split()\n",
    "        if len(words)==0:\n",
    "            with open(filename, 'r') as file:\n",
    "                words = file.read().split()\n",
    "        return set(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f8f2631-03f7-4393-a5d5-a539d150ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_words(inp, st_words):\n",
    "    A = listify(inp)\n",
    "\n",
    "    pp_c=0\n",
    "    pp = ['i', 'my', 'we','ours','us','I','My','We','Ours','Us']\n",
    "    for i in pp:\n",
    "        if i in A and i!='US':\n",
    "            pp_c+=1\n",
    "\n",
    "    \n",
    "    B = listify(st_words)\n",
    "    filtered = A-B\n",
    "\n",
    "    word_c=len(A)-len(filtered)\n",
    "    try:\n",
    "            \n",
    "        with open('Filtered/'+inp, 'w') as file:\n",
    "            file.write('\\n'.join(filtered))\n",
    "    except UnicodeEncodeError:\n",
    "        with open('Filtered/'+inp, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(filtered))\n",
    "\n",
    "    ###############################################################\n",
    "    #Calculating Positive, Negative, words\n",
    "    try:\n",
    "        with open('Filtered/'+inp, 'r') as file:\n",
    "            g=file.read().split()\n",
    "    except UnicodeDecodeError:\n",
    "        with open('Filtered/'+inp, 'r', encoding='utf-8') as file:\n",
    "            g=file.read().split()\n",
    "    no_pos, no_neg=0,0\n",
    "    for i in g:\n",
    "        if i in pos:\n",
    "            no_pos+=1\n",
    "        if i in neg:\n",
    "            no_neg+=1\n",
    "    polarity_score=(no_pos-no_neg)/((no_pos+no_neg)+0.000001)\n",
    "    subjectivity_score=(no_pos+no_neg)/(len(filtered)+0.000001)\n",
    "\n",
    "    return word_c, no_pos, no_neg, polarity_score,subjectivity_score, pp_c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2909697-4103-4c9b-a013-f7db7271989d",
   "metadata": {},
   "source": [
    "### **Calculate_metrics** gives the following analysed values after removing the common words-\n",
    "- avg sentence length\n",
    "- complex word count\n",
    "- avg word length\n",
    "- personal pronoun count\n",
    "- avg_syllables_per_word\n",
    "- avg_words_per_sentence\n",
    "- syllables_per_word\n",
    "- fog_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70bc1a04-1155-4a30-9263-833de72bf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as infile:\n",
    "            text = infile.read()\n",
    "    except:\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            text = infile.read()\n",
    "\n",
    "    \n",
    "    if len(text)==0:\n",
    "        return 0,0,0,0,0,0,0,0,0\n",
    "    else:\n",
    "            \n",
    "        syllables_per_word=0\n",
    "        for i in \"aeiouAEIOU\":\n",
    "            syllables_per_word+=text.count(i)\n",
    "        syllables_per_word-=text.count('ed')+text.count('es')\n",
    "    \n",
    "        \n",
    "        sentences = re.split(r'[.!?]', text)\n",
    "        sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "        dic = pyphen.Pyphen(lang='en')\n",
    "    \n",
    "        total_sentences = len(sentences)\n",
    "        total_words = 0\n",
    "        total_complex_words = 0\n",
    "        total_word_length = 0\n",
    "        total_personal_pronouns = 0\n",
    "        total_syllables = 0\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            total_words += len(words)\n",
    "            total_word_length += sum(len(word) for word in words)\n",
    "    \n",
    "            for word in words:\n",
    "                syllable_count = len(dic.inserted(word).split('-'))\n",
    "                total_syllables += syllable_count\n",
    "    \n",
    "                if syllable_count > 2:\n",
    "                    total_complex_words += 1\n",
    "    \n",
    "        #print(total_words)\n",
    "        \n",
    "        average_sentence_length = total_words / total_sentences if total_sentences > 0 else 1\n",
    "        complex_word_percentage = (total_complex_words)\n",
    "        average_word_length = total_word_length / total_words if total_words > 0 else 1\n",
    "        fog_index=0.4*(average_sentence_length+complex_word_percentage)\n",
    "        \n",
    "        average_syllables_per_word = total_syllables / total_words if total_words > 0 else 1\n",
    "        avg_words_per_sentence=total_words/len(sentences) if len(sentences)>0 else 1\n",
    "    \n",
    "        return average_sentence_length, complex_word_percentage,average_word_length,personal_pronoun,average_syllables_per_word,avg_words_per_sentence,syllables_per_word/total_words,fog_index, total_complex_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f38168-4af2-4623-b23f-0c7994bcc3e8",
   "metadata": {},
   "source": [
    "## **Uploader** is a function that will update the values of output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a81076-258b-4922-849a-1f6576a03eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uploader(x,l,file_path):\n",
    " \n",
    "    workbook = op.load_workbook(file_path)\n",
    "    sheet = workbook.active\n",
    "    for i in range(13):\n",
    "        a=sheet.cell(row=x, column=i+3)\n",
    "        a.value=l[i]\n",
    "        \n",
    "    workbook.save('Docs/Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5a9da-3c83-43d0-a299-7d64fb511774",
   "metadata": {},
   "source": [
    "## Calling the functions in the following order \n",
    "1. text_extractor\n",
    "2. Remove_Common_words\n",
    "3. Calculate_metrics\n",
    "4. Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e731f4-34d6-4f6a-9ca5-73d602420838",
   "metadata": {},
   "source": [
    "### Calling Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e9698b-8f21-4f45-94eb-dcd738a595da",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=pd.read_excel(\"Docs/Input.xlsx\")\n",
    "\n",
    "url=list(f.URL)\n",
    "url_id=list(f.URL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec84867-8406-48d6-b93c-12c409f77c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(url)):\n",
    "    text_extractor(url[i], str(url_id[i])+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabb0a0d-24d6-4efd-bf7d-59361682e37d",
   "metadata": {},
   "source": [
    "### Calling Remove_common_words and Calculate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fb73e2d-612e-489e-8d60-a87f6a87ffbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "stop_words = 'Docs/StopWords.txt'\n",
    "x=2\n",
    "for i in url_id:\n",
    "    #print(i)\n",
    "    if i!=44:\n",
    "        input_file = str(i)+'.txt'\n",
    "        word_count,positive_words, negative_words, polarity_score, subjectivity_score, personal_pronoun = remove_common_words(input_file, stop_words)\n",
    "        inp = 'Filtered/'+input_file\n",
    "        \n",
    "        avg_sentence_length, complex_word_percentage, avg_word_length, personal_pronoun, avg_syllables_per_word, avg_words_per_sentence, syllables_per_word, fog_index, total_complex_words= calculate_metrics(inp)\n",
    "        l=[positive_words,negative_words,polarity_score,subjectivity_score,avg_sentence_length,complex_word_percentage,fog_index,avg_words_per_sentence,total_complex_words, word_count,syllables_per_word, personal_pronoun, avg_word_length]\n",
    "        #uploading the derived values into the csv file\n",
    "        uploader(x, l, \"Docs/Output Data Structure.xlsx\")\n",
    "        x+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a99482d-ce63-440a-82dd-a2ad5b760190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
